{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T17:28:49.103165500Z",
     "start_time": "2023-11-12T17:28:40.798900800Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import time \n",
    "import PIL\n",
    "from IPython import display\n",
    "\n",
    "def get_training_data(image_path):\n",
    "    \"\"\"Loads an image from the given path into a numpy array.\"\"\"\n",
    "    with Image.open(image_path) as img:\n",
    "        return np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f564f4bcfca454fa",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:30:01.342981500Z",
     "start_time": "2023-11-12T17:30:01.295288400Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./mTBI/eye_motion_trace/00029_U_4_19_2018_9_18_9_V001.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8ac1d16ef2b9ad",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Actual project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e940f9eb26493cfb",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:30:03.899717300Z",
     "start_time": "2023-11-12T17:30:03.865058600Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_frame(video_path, time_in_seconds):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file.\")\n",
    "        return None\n",
    "\n",
    "    # Get the frame rate of the video\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Calculate the frame number to be captured\n",
    "    frame_number = int(time_in_seconds * fps)\n",
    "\n",
    "    # Set the frame position\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "\n",
    "    # Read the frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "\n",
    "    if ret:\n",
    "        return frame  # Return the frame if successfully captured\n",
    "    else:\n",
    "        print(\"Error: Could not retrieve frame at given time.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d3ec4acedf681c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Extracting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b50e3376866d84c6",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:30:09.846416900Z",
     "start_time": "2023-11-12T17:30:09.828651600Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_combine(arrays):\n",
    "    grayscaled = [np.mean(array, axis=2, keepdims=True) for array in arrays]\n",
    "    combined = np.stack(grayscaled, axis=0)\n",
    "    \n",
    "    # list of all the frames\n",
    "    \n",
    "    return combined\n",
    "    # extract_frame('./mTBI/video_sequence/00029_U_4_19_2018_9_18_9_V001.avi', 1).shape\n",
    "\n",
    "def avi_parser(avi_file=\"./video_sequence/00029_U_4_19_2018_9_18_9_V001.avi\"):\n",
    "    temp = []\n",
    "    \n",
    "    # avi_file = \"00029_U_4_19_2018_9_18_9_V001.avi\"\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(avi_file)\n",
    "    \n",
    "    # Check if the video file opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file.\")\n",
    "    else:\n",
    "        # Loop through each frame\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break  # Break the loop if there are no frames left\n",
    "    \n",
    "            # Process the frame here\n",
    "            # For example, you can display the frame using cv2.imshow(\"Frame\", frame)\n",
    "            # print(type(frame))\n",
    "            temp.append(frame)\n",
    "    \n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8320974ce446ca3c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:30:27.733942600Z",
     "start_time": "2023-11-12T17:30:27.154773200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(512, 512, 3)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avi_obj = avi_parser(\"./mTBI/video_sequence/00029_U_4_19_2018_9_18_9_V001.avi\")\n",
    "avi_obj[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2aa4767e4ef996e6",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:30:35.682311900Z",
     "start_time": "2023-11-12T17:30:31.659970200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "numpy.ndarray"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avi_train = convert_combine(avi_obj)\n",
    "type(avi_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56bcf7ecb580475e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:30:37.852268900Z",
     "start_time": "2023-11-12T17:30:37.781274100Z"
    }
   },
   "outputs": [],
   "source": [
    "def shape_finder(csv_directory= './eye_motion_trace'):\n",
    "    # Directory containing CSV files\n",
    "    csv_directory = './eye_motion_trace'\n",
    "    \n",
    "    # Loop through each file in the directory\n",
    "    for filename in os.listdir(csv_directory):\n",
    "        if filename.endswith('.csv'):\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(csv_directory, filename)\n",
    "    \n",
    "            # Read the CSV file into a DataFrame\n",
    "            temp = pd.read_csv(file_path)\n",
    "    \n",
    "            # Print the filename and shape of the DataFrame\n",
    "            print(f\"File: {filename}, Shape: {temp.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "244416fbe94e5ed6",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:30:40.398881700Z",
     "start_time": "2023-11-12T17:30:39.186970700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(300, 512, 512, 1)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avi_train = avi_train.reshape(avi_train.shape[0], 512, 512, 1).astype('float32')\n",
    "avi_train_images = (avi_train - 127.5) / 127.5  # Normalize the images to [-1, 1]\n",
    "avi_train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af353ea73f45772f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Getting noise from vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8fc020823465b1f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:30:42.979148600Z",
     "start_time": "2023-11-12T17:30:42.945061Z"
    }
   },
   "outputs": [],
   "source": [
    "df_array = df.drop(columns=['time[s]']).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5036d26693502bd7",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:30:44.531941400Z",
     "start_time": "2023-11-12T17:30:44.472372900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-9.97400e-01,  2.69573e+01, -1.22000e-02, ...,  2.70000e+01,\n        -1.22000e-02,  2.47200e-01],\n       [-9.97600e-01,  2.43766e+01, -1.22000e-02, ..., -1.00000e+00,\n        -1.22000e-02, -9.20000e-03],\n       [-9.97900e-01,  2.18332e+01, -1.22000e-02, ..., -1.00000e+00,\n        -1.22000e-02, -9.20000e-03],\n       ...,\n       [-1.84842e+01, -2.28521e+01, -2.25600e-01, ..., -8.00000e+00,\n        -2.31900e-01, -7.32000e-02],\n       [-1.85297e+01, -2.39695e+01, -2.26200e-01, ..., -8.00000e+00,\n        -2.31900e-01, -7.32000e-02],\n       [-1.85716e+01, -2.50233e+01, -2.26700e-01, ..., -3.50000e+01,\n        -2.31900e-01, -3.20400e-01]])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8c515049ad0c7a7",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:30:46.106431900Z",
     "start_time": "2023-11-12T17:30:46.027285900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-9.97400000e-01,  2.69573000e+01, -1.22000000e-02, ...,\n         7.55038481e-01,  7.42196329e-01,  3.40122227e-01],\n       [-9.97600000e-01,  2.43766000e+01, -1.22000000e-02, ...,\n         1.51536337e-01,  1.99352702e-01,  9.37881290e-01],\n       [-9.97900000e-01,  2.18332000e+01, -1.22000000e-02, ...,\n         7.52243761e-01,  9.05998350e-01,  7.87899037e-01],\n       ...,\n       [-1.84842000e+01, -2.28521000e+01, -2.25600000e-01, ...,\n         2.37927160e-01,  4.05299722e-01,  8.98570925e-01],\n       [-1.85297000e+01, -2.39695000e+01, -2.26200000e-01, ...,\n         4.77025538e-01,  3.09136373e-01,  3.51113451e-01],\n       [-1.85716000e+01, -2.50233000e+01, -2.26700000e-01, ...,\n         4.10377451e-01,  6.18402064e-01,  7.63029714e-01]])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of rows in the original array\n",
    "num_rows = df_array.shape[0]\n",
    "\n",
    "# Generate noise (random numbers, for example)\n",
    "# Adjust the parameters of np.random.rand() as needed\n",
    "noise = np.random.rand(num_rows, 92)\n",
    "\n",
    "# Concatenate the original array and the noise array\n",
    "combined_array = np.concatenate([df_array, noise], axis=1)\n",
    "combined_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7e2d50173f9fac",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "620ab37d4386d976",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:30:57.420852600Z",
     "start_time": "2023-11-12T17:30:57.385955800Z"
    }
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68a7aba298160288",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:31:00.045356800Z",
     "start_time": "2023-11-12T17:30:59.864851700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Batch and shuffle the data\n",
    "data_set = tf.data.Dataset.from_tensor_slices(avi_train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating the generator and the discriminator"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9967d9183d47f35d"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__StatelessRandomUniformV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[262144,65536] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu [Op:StatelessRandomUniformV2]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mResourceExhaustedError\u001B[0m                    Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[34], line 92\u001B[0m\n\u001B[0;32m     89\u001B[0m autoencoder \u001B[38;5;241m=\u001B[39m make_autoencoder_model(input_dim)\n\u001B[0;32m     91\u001B[0m \u001B[38;5;66;03m# Create the generator\u001B[39;00m\n\u001B[1;32m---> 92\u001B[0m generator \u001B[38;5;241m=\u001B[39m \u001B[43mmake_generator_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_dim\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     94\u001B[0m \u001B[38;5;66;03m# Pretrain the generator using the autoencoder\u001B[39;00m\n\u001B[0;32m     95\u001B[0m \u001B[38;5;66;03m# Assuming your training data is in 'train_data'\u001B[39;00m\n\u001B[0;32m     96\u001B[0m \u001B[38;5;66;03m# Make sure to preprocess your data as needed (e.g., normalize)\u001B[39;00m\n\u001B[0;32m     97\u001B[0m train_data_flattened \u001B[38;5;241m=\u001B[39m avi_train_images\u001B[38;5;241m.\u001B[39mreshape(avi_train_images\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "Cell \u001B[1;32mIn[34], line 22\u001B[0m, in \u001B[0;36mmake_generator_model\u001B[1;34m(input_dim)\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmake_generator_model\u001B[39m(input_dim):\n\u001B[0;32m     21\u001B[0m     model \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mSequential()\n\u001B[1;32m---> 22\u001B[0m     \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlayers\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDense\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m8\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m8\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m1024\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_bias\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_shape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43minput_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     23\u001B[0m     model\u001B[38;5;241m.\u001B[39madd(layers\u001B[38;5;241m.\u001B[39mBatchNormalization())\n\u001B[0;32m     24\u001B[0m     model\u001B[38;5;241m.\u001B[39madd(layers\u001B[38;5;241m.\u001B[39mLeakyReLU())\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\GradientDescentProject\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001B[0m, in \u001B[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    203\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_self_setattr_tracking \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[0;32m    204\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 205\u001B[0m   result \u001B[38;5;241m=\u001B[39m method(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    206\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    207\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_self_setattr_tracking \u001B[38;5;241m=\u001B[39m previous_value  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\GradientDescentProject\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\GradientDescentProject\\lib\\site-packages\\keras\\backend.py:2100\u001B[0m, in \u001B[0;36mRandomGenerator.random_uniform\u001B[1;34m(self, shape, minval, maxval, dtype, nonce)\u001B[0m\n\u001B[0;32m   2098\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m nonce:\n\u001B[0;32m   2099\u001B[0m         seed \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mstateless_fold_in(seed, nonce)\n\u001B[1;32m-> 2100\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandom\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstateless_uniform\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2101\u001B[0m \u001B[43m        \u001B[49m\u001B[43mshape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2102\u001B[0m \u001B[43m        \u001B[49m\u001B[43mminval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mminval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2103\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmaxval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaxval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2104\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2105\u001B[0m \u001B[43m        \u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2106\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2107\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39muniform(\n\u001B[0;32m   2108\u001B[0m     shape\u001B[38;5;241m=\u001B[39mshape,\n\u001B[0;32m   2109\u001B[0m     minval\u001B[38;5;241m=\u001B[39mminval,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2112\u001B[0m     seed\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmake_legacy_seed(),\n\u001B[0;32m   2113\u001B[0m )\n",
      "\u001B[1;31mResourceExhaustedError\u001B[0m: {{function_node __wrapped__StatelessRandomUniformV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[262144,65536] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu [Op:StatelessRandomUniformV2]"
     ]
    }
   ],
   "source": [
    "# def make_autoencoder_model(input_dim):\n",
    "#     model = tf.keras.Sequential()\n",
    "#     model.add(layers.Dense(256, activation='relu', input_shape=(input_dim,)))\n",
    "#     model.add(layers.Dense(128, activation='relu'))\n",
    "#     model.add(layers.Dense(64, activation='relu'))\n",
    "#     model.add(layers.Dense(128, activation='relu'))\n",
    "#     model.add(layers.Dense(256, activation='relu'))\n",
    "#     model.add(layers.Dense(input_dim, activation='sigmoid'))\n",
    "#     return model\n",
    "# \n",
    "# def pretrain_generator(generator, autoencoder, data):\n",
    "#     # Assuming data is your training dataset\n",
    "#     autoencoder.compile(optimizer='adam', loss='mse')\n",
    "#     autoencoder.fit(data, data, epochs=10, batch_size=32)  # Adjust epochs and batch_size as needed\n",
    "# \n",
    "#     # Set the weights of the generator using the trained autoencoder\n",
    "#     generator.set_weights(autoencoder.get_weights())\n",
    "#     return generator\n",
    "# \n",
    "# def make_generator_model(input_dim):\n",
    "#     model = tf.keras.Sequential()\n",
    "#     model.add(layers.Dense(8*8*1024, use_bias=False, input_shape=(input_dim,)))\n",
    "#     model.add(layers.BatchNormalization())\n",
    "#     model.add(layers.LeakyReLU())\n",
    "# \n",
    "#     model.add(layers.Reshape((8, 8, 1024)))\n",
    "#     # Size becomes 8x8x1024 here\n",
    "# \n",
    "#     # Upsample to 16x16\n",
    "#     model.add(layers.Conv2DTranspose(512, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "#     model.add(layers.BatchNormalization())\n",
    "#     model.add(layers.LeakyReLU())\n",
    "#     # Size becomes 16x16x512 here\n",
    "# \n",
    "#     # Upsample to 32x32\n",
    "#     model.add(layers.Conv2DTranspose(256, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "#     model.add(layers.BatchNormalization())\n",
    "#     model.add(layers.LeakyReLU())\n",
    "#     # Size becomes 32x32x256 here\n",
    "# \n",
    "#     # Upsample to 64x64\n",
    "#     model.add(layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "#     model.add(layers.BatchNormalization())\n",
    "#     model.add(layers.LeakyReLU())\n",
    "#     # Size becomes 64x64x128 here\n",
    "# \n",
    "#     # Upsample to 128x128\n",
    "#     model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "#     model.add(layers.BatchNormalization())\n",
    "#     model.add(layers.LeakyReLU())\n",
    "#     # Size becomes 128x128x64 here\n",
    "# \n",
    "#     # Upsample to 256x256\n",
    "#     model.add(layers.Conv2DTranspose(32, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "#     model.add(layers.BatchNormalization())\n",
    "#     model.add(layers.LeakyReLU())\n",
    "#     # Size becomes 256x256x32 here\n",
    "# \n",
    "#     # Upsample to 512x512\n",
    "#     model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "#     # Final size becomes 512x512x1\n",
    "# \n",
    "#     return model\n",
    "# \n",
    "# def make_discriminator_model(input_dim=512*512):\n",
    "#     model = tf.keras.Sequential()\n",
    "#     model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[512, 512, 1]))\n",
    "#     model.add(layers.LeakyReLU())\n",
    "#     model.add(layers.Dropout(0.3))\n",
    "# \n",
    "#     # Add a MaxPooling2D layer\n",
    "#     model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "# \n",
    "#     model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "#     model.add(layers.LeakyReLU())\n",
    "#     model.add(layers.Dropout(0.3))\n",
    "# \n",
    "#     # Add a GlobalAveragePooling2D layer\n",
    "#     model.add(layers.GlobalAveragePooling2D())\n",
    "# \n",
    "#     model.add(layers.Dense(1))\n",
    "# \n",
    "#     return model\n",
    "# \n",
    "# # Assuming your data has shape (num_samples, input_dim)\n",
    "# input_dim = 512*512  # Adjust based on your actual input shape\n",
    "# \n",
    "# # Create the autoencoder\n",
    "# autoencoder = make_autoencoder_model(input_dim)\n",
    "# \n",
    "# # Create the generator\n",
    "# generator = make_generator_model(input_dim)\n",
    "# \n",
    "# # Pretrain the generator using the autoencoder\n",
    "# # Assuming your training data is in 'train_data'\n",
    "# # Make sure to preprocess your data as needed (e.g., normalize)\n",
    "# train_data_flattened = avi_train_images.reshape(avi_train_images.shape[0], -1)\n",
    "# generator = pretrain_generator(generator, autoencoder, train_data_flattened)\n",
    "# \n",
    "# # Create the discriminator\n",
    "# discriminator = make_discriminator_model()\n",
    "# \n",
    "# # Continue with the GAN training, combining generator and discriminator\n",
    "# # ...\n",
    "# \n",
    "# # Compile and train the GAN\n",
    "# # ...\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T18:19:15.410074400Z",
     "start_time": "2023-11-12T18:19:01.869688800Z"
    }
   },
   "id": "7dbb993dea872199"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Previous attempt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a21e616c89b0f49"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "\n",
    "def make_encoder_model():\n",
    "    encoder = tf.keras.Sequential()\n",
    "    encoder.add(layers.Dense(8*8*1024, use_bias=False, input_shape=(100,)))\n",
    "    encoder.add(layers.BatchNormalization())\n",
    "    encoder.add(layers.LeakyReLU())\n",
    "    encoder.add(layers.Reshape((8, 8, 1024)))\n",
    "    encoder.add(layers.Conv2DTranspose(512, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    encoder.add(layers.BatchNormalization())\n",
    "    encoder.add(layers.LeakyReLU())\n",
    "    encoder.add(layers.Conv2DTranspose(256, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    encoder.add(layers.BatchNormalization())\n",
    "    encoder.add(layers.LeakyReLU())\n",
    "    encoder.add(layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    encoder.add(layers.BatchNormalization())\n",
    "    encoder.add(layers.LeakyReLU())\n",
    "    encoder.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    encoder.add(layers.BatchNormalization())\n",
    "    encoder.add(layers.LeakyReLU())\n",
    "    encoder.add(layers.Conv2DTranspose(32, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    encoder.add(layers.BatchNormalization())\n",
    "    encoder.add(layers.LeakyReLU())\n",
    "    return encoder\n",
    "\n",
    "def make_decoder_model():\n",
    "    decoder = tf.keras.Sequential()\n",
    "    decoder.add(layers.Conv2DTranspose(32, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    decoder.add(layers.BatchNormalization())\n",
    "    decoder.add(layers.LeakyReLU())\n",
    "    decoder.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    decoder.add(layers.BatchNormalization())\n",
    "    decoder.add(layers.LeakyReLU())\n",
    "    decoder.add(layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    decoder.add(layers.BatchNormalization())\n",
    "    decoder.add(layers.LeakyReLU())\n",
    "    decoder.add(layers.Conv2DTranspose(256, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    decoder.add(layers.BatchNormalization())\n",
    "    decoder.add(layers.LeakyReLU())\n",
    "    decoder.add(layers.Conv2DTranspose(512, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    decoder.add(layers.BatchNormalization())\n",
    "    decoder.add(layers.LeakyReLU())\n",
    "    decoder.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    return decoder\n",
    "\n",
    "def make_autoencoder_model():\n",
    "    # Create encoder and decoder instances\n",
    "    encoder = make_encoder_model()\n",
    "    decoder = make_decoder_model()\n",
    "\n",
    "    # Define input layer\n",
    "    input_img = layers.Input(shape=(100,))\n",
    "\n",
    "    # Connect the input to the encoder and decoder\n",
    "    encoded_img = encoder(input_img)\n",
    "    decoded_img = decoder(encoded_img)\n",
    "\n",
    "    # Create the autoencoder model\n",
    "    autoencoder = Model(inputs=input_img, outputs=decoded_img)\n",
    "\n",
    "    return autoencoder\n",
    "\n",
    "# Create an instance of the autoencoder model\n",
    "autoencoder_model = make_autoencoder_model()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T18:27:43.608392200Z",
     "start_time": "2023-11-12T18:27:42.030119500Z"
    }
   },
   "id": "110481af2aa03e48"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b69f40301d3a01fe",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:37:47.590498200Z",
     "start_time": "2023-11-12T17:37:47.550590700Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    # Start with a dense layer that reshapes into a 8x8x1024 tensor\n",
    "    model.add(layers.Dense(8*8*1024, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((8, 8, 1024)))\n",
    "    # Size becomes 8x8x1024 here\n",
    "\n",
    "    # Upsample to 16x16\n",
    "    model.add(layers.Conv2DTranspose(512, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    # Size becomes 16x16x512 here\n",
    "\n",
    "    # Upsample to 32x32\n",
    "    model.add(layers.Conv2DTranspose(256, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    # Size becomes 32x32x256 here\n",
    "\n",
    "    # Upsample to 64x64\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    # Size becomes 64x64x128 here\n",
    "\n",
    "    # Upsample to 128x128\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    # Size becomes 128x128x64 here\n",
    "\n",
    "    # Upsample to 256x256\n",
    "    model.add(layers.Conv2DTranspose(32, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    # Size becomes 256x256x32 here\n",
    "\n",
    "    # Upsample to 512x512\n",
    "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    # Final size becomes 512x512x1\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6230a74fe1a435d3",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T17:37:51.054346200Z",
     "start_time": "2023-11-12T17:37:48.856928700Z"
    }
   },
   "outputs": [],
   "source": [
    "generator = make_generator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3741eea94522d5b1",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T18:29:15.140416800Z",
     "start_time": "2023-11-12T18:29:15.122436300Z"
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "# def make_discriminator_model():\n",
    "#     model = tf.keras.Sequential()\n",
    "#     model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "#                                      input_shape=[512, 512, 1]))\n",
    "#     model.add(layers.LeakyReLU())\n",
    "#     model.add(layers.Dropout(0.3))\n",
    "# \n",
    "#     model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "#     model.add(layers.LeakyReLU())\n",
    "#     model.add(layers.Dropout(0.3))\n",
    "# \n",
    "#     model.add(layers.Flatten())\n",
    "#     model.add(layers.Dense(1))\n",
    "# \n",
    "#     return model\n",
    "\n",
    "######################\n",
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[512, 512, 1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # Add a MaxPooling2D layer\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # Add a GlobalAveragePooling2D layer\n",
    "    model.add(layers.GlobalAveragePooling2D())\n",
    "\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e2a3746c54151e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T18:29:16.771536300Z",
     "start_time": "2023-11-12T18:29:16.692551400Z"
    }
   },
   "outputs": [],
   "source": [
    "discriminator = make_discriminator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eef66a5ac39960ef",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T18:29:17.630074300Z",
     "start_time": "2023-11-12T18:29:17.625862400Z"
    }
   },
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bfecdf220a4cd7fd",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T18:29:20.226189700Z",
     "start_time": "2023-11-12T18:29:20.210548400Z"
    }
   },
   "outputs": [],
   "source": [
    "# descrim loss function\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "42230804b126c47c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T18:29:21.053022200Z",
     "start_time": "2023-11-12T18:29:21.038501100Z"
    }
   },
   "outputs": [],
   "source": [
    "# generator loss\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7fe77c12a4ae6430",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T18:29:26.155986300Z",
     "start_time": "2023-11-12T18:29:26.137694200Z"
    }
   },
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "52f628d41db80919",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T18:29:30.451354500Z",
     "start_time": "2023-11-12T18:29:30.438219600Z"
    }
   },
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d60dfd70b4da5256",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T18:29:33.310925200Z",
     "start_time": "2023-11-12T18:29:33.283016900Z"
    }
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 1\n",
    "\n",
    "# You will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = combined_array[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d2e4e3504790d15",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T18:30:11.314481300Z",
     "start_time": "2023-11-12T18:30:11.293734Z"
    }
   },
   "outputs": [],
   "source": [
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_images = generator(noise, training=True)\n",
    "      print(images.shape)\n",
    "      real_output = discriminator(images, training=True)\n",
    "      print('DEBUB NOISE:', noise)\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f0852a5a2bda8928",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T18:30:11.787568300Z",
     "start_time": "2023-11-12T18:30:11.772843900Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    for image_batch in dataset:\n",
    "      train_step(image_batch)\n",
    "    # Produce images for the GIF as you go\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(autoencoder_model,\n",
    "                             epoch + 1,\n",
    "                             seed)\n",
    "    # Save the model every 15 epochs\n",
    "    if (epoch + 1) % 15 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "  # Generate after the final epoch\n",
    "  display.clear_output(wait=True)\n",
    "  generate_and_save_images(autoencoder_model,\n",
    "                           epochs,\n",
    "                           seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e2a3e33ccdb29ad3",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T18:30:14.002931500Z",
     "start_time": "2023-11-12T18:30:13.975657Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "  predictions = model(test_input, training=False)\n",
    "\n",
    "  fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(4, 4, i+1)\n",
    "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "      plt.axis('off')\n",
    "\n",
    "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TRAINING MODEL (DANGER)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98ecb477ee53bc00"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57291fbbd8922d0a",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-12T18:30:18.657195500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 512, 512, 1)\n",
      "DEBUB NOISE: Tensor(\"random_normal:0\", shape=(256, 100), dtype=float32)\n",
      "(256, 512, 512, 1)\n",
      "DEBUB NOISE: Tensor(\"random_normal:0\", shape=(256, 100), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train(data_set, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc68a440041d891",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fc8e9fc5a17eee",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Display a single image using the epoch number\n",
    "def display_image(epoch_no):\n",
    "  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ed40286770e88e",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "display_image(EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
