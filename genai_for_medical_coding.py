# -*- coding: utf-8 -*-
"""GenAI For Medical Coding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sEIBhHGcpypBIHYZ34rsLzHAlzN4VRGx
"""

import matplotlib.pyplot as plt
import seaborn as sns
import keras
from keras import layers
from keras.models import Sequential
from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization
from keras.layers import Dense, Reshape, Flatten, Conv2D, Conv2DTranspose, ReLU, LeakyReLU, Dropout # adding layers to the Neural Network model
from tensorflow import keras # for building Neural Networks
from tensorflow.keras.utils import plot_model # for plotting model diagram
from tensorflow.keras.optimizers import Adam # for model optimization

from sklearn.preprocessing import MinMaxScaler # for scaling inputs used in the generator and discriminator

from keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report,confusion_matrix
from keras.callbacks import ReduceLROnPlateau
import cv2
import os
import tensorflow as tf
import pandas as pd
from PIL import Image
import numpy as np
import time
import PIL
from IPython import display

def get_training_data(image_path):
    """Loads an image from the given path into a numpy array."""
    with Image.open(image_path) as img:
        return np.array(img)

"""# Actual project"""

def extract_frame(video_path, time_in_seconds):
    # Open the video file
    cap = cv2.VideoCapture(video_path)

    if not cap.isOpened():
        print("Error: Could not open video file.")
        return None

    # Get the frame rate of the video
    fps = cap.get(cv2.CAP_PROP_FPS)

    # Calculate the frame number to be captured
    frame_number = int(time_in_seconds * fps)

    # Set the frame position
    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)

    # Read the frame
    ret, frame = cap.read()

    # Release the video capture object
    cap.release()

    if ret:
        return frame  # Return the frame if successfully captured
    else:
        print("Error: Could not retrieve frame at given time.")
        return None

"""### Extracting data"""

def convert_combine(arrays):
    grayscaled = [np.mean(array, axis=2, keepdims=True) for array in arrays]
    combined = np.stack(grayscaled, axis=0)

    # list of all the frames

    return combined
    # extract_frame('./mTBI/video_sequence/00029_U_4_19_2018_9_18_9_V001.avi', 1).shape

def avi_parser(avi_file="./mTBI/video_sequence/00029_U_4_19_2018_9_18_9_V001.avi"):
    temp = []

    # avi_file = "00029_U_4_19_2018_9_18_9_V001.avi"
    # Open the video file
    cap = cv2.VideoCapture(avi_file)

    # Check if the video file opened successfully
    if not cap.isOpened():
        print("Error: Could not open video file.")
    else:
        # Loop through each frame
        while True:
            ret, frame = cap.read()
            if not ret:
                break  # Break the loop if there are no frames left

            # Process the frame here
            # For example, you can display the frame using cv2.imshow("Frame", frame)
            # print(type(frame))
            temp.append(frame)

    # Release the video capture object
    cap.release()
    cv2.destroyAllWindows()
    return temp

avi_obj = avi_parser("00233_U_9_6_2018_13_12_13_V001.avi")
# avi_obj = avi_parser("./mTBI/video_sequence/00233_U_9_27_2018_14_52_33_V001.avi")
# print(np.array(temp).shape)

# temp = avi_parser("./mTBI/video_sequence/00152_U_5_16_2018_8_49_17_V001.avi")
# # print(np.array(temp).shape)
# avi_obj += temp

# temp = avi_parser("./mTBI/video_sequence/00029_U_4_19_2018_9_18_9_V001.avi")
# # print(np.array(temp).shape)
# avi_obj += temp

print(np.array(avi_obj).shape)

avi_train = convert_combine(avi_obj)
type(avi_train)

def shape_finder(csv_directory= './mTBI/eye_motion_trace'):
    # Directory containing CSV files
    csv_directory = './mTBI/eye_motion_trace'

    # Loop through each file in the directory
    for filename in os.listdir(csv_directory):
        if filename.endswith('.csv'):
            # Construct the full file path
            file_path = os.path.join(csv_directory, filename)

            # Read the CSV file into a DataFrame
            temp = pd.read_csv(file_path)

            # Print the filename and shape of the DataFrame
            print(f"File: {filename}, Shape: {temp.shape[0]}")

avi_train = avi_train.reshape(avi_train.shape[0], 512, 512, 1).astype('float32')
avi_train_images = (avi_train - 127.5) / 127.5  # Normalize the images to [-1, 1]
avi_train_images.shape

fig, axs = plt.subplots(2, 5, sharey=False, tight_layout=True, figsize=(16,9), facecolor='white')
n=0
for i in range(0,2):
    for j in range(0,5):
        axs[i,j].matshow(avi_train_images[n + 200])
        n=n+1
plt.show()

# We don't have the computational power to process 300 512x512 images
def downscale_image(image):
    # Assuming image is a numpy array of shape (512, 512, 1)
    # Resize the image to 64x64
    downscaled_image = cv2.resize(image, (64, 64), interpolation=cv2.INTER_AREA)
    return downscaled_image

temp = []
for i in range(len(avi_train_images)):
    if i % 2 != 0:
        continue
    temp.append(downscale_image(avi_train_images[i]))

fig, axs = plt.subplots(2, 5, sharey=False, tight_layout=True, figsize=(16,9), facecolor='white')
n=0
for i in range(0,2):
    for j in range(0,5):
        axs[i,j].matshow(temp[n])
        n=n+1
plt.show()

avi_train_images = np.array(temp)

# Scaler
scaler=MinMaxScaler(feature_range=(-1, 1))

# Select images that we want to use fro model trainng
data=avi_train_images.copy()
print("Original shape of the data: ", data.shape)

# Reshape array
data=data.reshape(-1, 1)
print("Reshaped data: ", data.shape)

# Fit the scaler
scaler.fit(data)

# Scale the array
data=scaler.transform(data)

# Reshape back to the original shape
data=data.reshape(avi_train_images.shape[0], 64, 64, 1)
print("Shape of the scaled array: ", data.shape)

"""# Side quest"""

BUFFER_SIZE = 60000
BATCH_SIZE = 256

# Batch and shuffle the data
data_set = tf.data.Dataset.from_tensor_slices(avi_train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)

def generator(latent_dim):
    model = Sequential(name="Generator") # Model

    # Hidden Layer 1: Start with 8 x 8 image
    n_nodes = 8 * 8 * 128 # number of nodes in the first hidden layer
    model.add(Dense(n_nodes, input_dim=latent_dim, name='Generator-Hidden-Layer-1'))
    model.add(Reshape((8, 8, 128), name='Generator-Hidden-Layer-Reshape-1'))

    # Hidden Layer 2: Upsample to 16 x 16
    model.add(Conv2DTranspose(filters=128, kernel_size=(4,4), strides=(2,2), padding='same', name='Generator-Hidden-Layer-2'))
    model.add(ReLU(name='Generator-Hidden-Layer-Activation-2'))

    # Hidden Layer 3: Upsample to 32 x 32
    model.add(Conv2DTranspose(filters=256, kernel_size=(4,4), strides=(2,2), padding='same', name='Generator-Hidden-Layer-3'))
    model.add(ReLU(name='Generator-Hidden-Layer-Activation-3'))

    # Hidden Layer 4: Upsample to 64 x 64
    model.add(Conv2DTranspose(filters=512, kernel_size=(4,4), strides=(2,2), padding='same', name='Generator-Hidden-Layer-4'))
    model.add(ReLU(name='Generator-Hidden-Layer-Activation-4'))

    model.add(Conv2D(filters=1, kernel_size=(5,5), activation='tanh', padding='same', name='Generator-Output-Layer'))
    return model

# Instantiate
latent_dim=100 # Our latent space has 100 dimensions. We can change it to any number
gen_model = generator(latent_dim)
# plot_model(gen_model, show_shapes=True, show_layer_names=True, dpi=400)

def discriminator(in_shape=(64,64,1)):
    model = Sequential(name="Discriminator") # Model

    # Hidden Layer 1
    model.add(Conv2D(filters=64, kernel_size=(4,4), strides=(2, 2), padding='same', input_shape=in_shape, name='Discriminator-Hidden-Layer-1'))
    model.add(LeakyReLU(alpha=0.2, name='Discriminator-Hidden-Layer-Activation-1'))

    # Hidden Layer 2
    model.add(Conv2D(filters=128, kernel_size=(4,4), strides=(2, 2), padding='same', input_shape=in_shape, name='Discriminator-Hidden-Layer-2'))
    model.add(LeakyReLU(alpha=0.2, name='Discriminator-Hidden-Layer-Activation-2'))

    # Hidden Layer 3
    model.add(Conv2D(filters=128, kernel_size=(4,4), strides=(2, 2), padding='same', input_shape=in_shape, name='Discriminator-Hidden-Layer-3'))
    model.add(LeakyReLU(alpha=0.2, name='Discriminator-Hidden-Layer-Activation-3'))

    # Flatten and Output Layers
    model.add(Flatten(name='Discriminator-Flatten-Layer')) # Flatten the shape
    model.add(Dropout(0.3, name='Discriminator-Flatten-Layer-Dropout')) # Randomly drop some connections for better generalization
    model.add(Dense(1, activation='sigmoid', name='Discriminator-Output-Layer')) # Output Layer

    # Compile the model
    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5), metrics=['accuracy'])
    return model

# Instantiate
dis_model = discriminator()

# Show model summary and plot model diagram
dis_model.summary()
# plot_model(dis_model, show_shapes=True, show_layer_names=True, dpi=400)

def def_gan(generator, discriminator):

    # We don't want to train the weights of discriminator at this stage. Hence, make it not trainable
    discriminator.trainable = False

    # Combine
    model = Sequential(name="DCGAN") # GAN Model
    model.add(generator) # Add Generator
    model.add(discriminator) # Add Disriminator

    # Compile the model
    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5))
    return model

# Instantiate
gan_model = def_gan(gen_model, dis_model)

gan_model.summary()

def real_samples(n, dataset):

    # Samples of real data
    X = dataset[np.random.choice(dataset.shape[0], n, replace=True), :]

    # Class labels
    y = np.ones((n, 1))
    return X, y


def latent_vector(latent_dim, n):

    # Generate points in the latent space
    latent_input = np.random.randn(latent_dim * n)

    # Reshape into a batch of inputs for the network
    latent_input = latent_input.reshape(n, latent_dim)
    return latent_input


def fake_samples(generator, latent_dim, n):

    # Generate points in latent space
    latent_output = latent_vector(latent_dim, n)

    # Predict outputs (i.e., generate fake samples)
    X = generator.predict(latent_output)

    # Create class labels
    y = np.zeros((n, 1))
    return X, y

def performance_summary(generator, discriminator, dataset, latent_dim, n=50):

    # Get samples of the real data
    x_real, y_real = real_samples(n, dataset)
    # Evaluate the descriminator on real data
    _, real_accuracy = discriminator.evaluate(x_real, y_real, verbose=0)

    # Get fake (generated) samples
    x_fake, y_fake = fake_samples(generator, latent_dim, n)
    # Evaluate the descriminator on fake (generated) data
    _, fake_accuracy = discriminator.evaluate(x_fake, y_fake, verbose=0)

    # summarize discriminator performance
    print("*** Evaluation ***")
    print("Discriminator Accuracy on REAL images: ", real_accuracy)
    print("Discriminator Accuracy on FAKE (generated) images: ", fake_accuracy)

    # Display 6 fake images
    x_fake_inv_trans=x_fake.reshape(-1, 1)
    x_fake_inv_trans=scaler.inverse_transform(x_fake_inv_trans)
    x_fake_inv_trans=x_fake_inv_trans.reshape(n, 64, 64, 1)

    fig, axs = plt.subplots(2, 3, sharey=False, tight_layout=True, figsize=(12,6), facecolor='white')
    k=0
    for i in range(0,2):
        for j in range(0,3):
            axs[i,j].matshow(x_fake_inv_trans[k])
            k=k+1
    plt.show()

def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=1001, n_batch=32, n_eval=100):

    # Our batch to train the discriminator will consist of half real images and half fake (generated) images
    half_batch = int(n_batch / 2)

    # We will manually enumare epochs
    for i in range(n_epochs):

    # Discriminator training
        # Prep real samples
        x_real, y_real = real_samples(half_batch, dataset)
        # Prep fake (generated) samples
        x_fake, y_fake = fake_samples(g_model, latent_dim, half_batch)

        # Train the discriminator using real and fake samples
        X, y = np.vstack((x_real, x_fake)), np.vstack((y_real, y_fake))
        discriminator_loss, _ = d_model.train_on_batch(X, y)

    # Generator training
        # Get values from the latent space to be used as inputs for the generator
        x_gan = latent_vector(latent_dim, n_batch)
        # While we are generating fake samples,
        # we want GAN generator model to create examples that resemble the real ones,
        # hence we want to pass labels corresponding to real samples, i.e. y=1, not 0.
        y_gan = np.ones((n_batch, 1))

        # Train the generator via a composite GAN model
        generator_loss = gan_model.train_on_batch(x_gan, y_gan)

        # Evaluate the model at every n_eval epochs
        if (i) % n_eval == 0:
            print("Epoch number: ", i)
            print("*** Training ***")
            print("Discriminator Loss ", discriminator_loss)
            print("Generator Loss: ", generator_loss)
            performance_summary(g_model, d_model, dataset, latent_dim)

tf.keras.utils.disable_interactive_logging()

# print(dataset.shape)
train(gen_model, dis_model, gan_model, data, latent_dim)

train(gen_model, dis_model, gan_model, data, latent_dim)

train(gen_model, dis_model, gan_model, data, latent_dim)

# Generate an image of a bonsai tree using random latent vector
x_fake, y_fake = fake_samples(gen_model, 100, 1)

# Shape and inverse transpose the generated image
x_fake_inv_trans=x_fake.reshape(-1, 1)
x_fake_inv_trans=scaler.inverse_transform(x_fake_inv_trans)
x_fake_inv_trans=x_fake_inv_trans.reshape(64, 64, 1)

# Show the generated image
fig, ax = plt.subplots(figsize=(16,9), dpi=20)
ax.matshow(x_fake_inv_trans)
plt.axis('off')
plt.show()

print(x_fake_inv_trans.shape)

all_frames = []

tot_frames = 3000
for i in range(tot_frames):
  x_fake, y_fake = fake_samples(gen_model, 100, 1)
  x_fake_inv_trans=x_fake.reshape(-1, 1)
  x_fake_inv_trans=scaler.inverse_transform(x_fake_inv_trans)
  x_fake_inv_trans=x_fake_inv_trans.reshape(64, 64, 1)

  all_frames.append(x_fake_inv_trans)

  if i % 100 == 0:
    print(i)

import cv2
import numpy as np

def create_video(image_list, output_file='output.avi', fps=30):
    """
    Creates a video from a list of grayscale images (numpy arrays).

    :param image_list: List of numpy arrays, each representing a grayscale image.
    :param output_file: Name of the output video file.
    :param fps: Frames per second of the output video.
    """
    # Check the shape of the first image to determine if it's 2D or 3D
    if len(image_list[0].shape) == 3:
        height, width, _ = image_list[0].shape
    else:  # It's a 2D image
        height, width = image_list[0].shape

    # Define the codec and create VideoWriter object
    fourcc = cv2.VideoWriter_fourcc(*'XVID')
    out = cv2.VideoWriter(output_file, fourcc, fps, (width, height), False)

    for img in image_list:
        if img.dtype != np.uint8:
            # Normalize and convert to uint8
            img = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)

        if len(img.shape) == 3 and img.shape[2] == 1:  # If the image is 3D but has only one channel
            img = img.reshape((height, width))  # Convert it to 2D

        if img.ndim == 2:  # If the image is 2D
            img = np.stack((img,)*3, axis=-1)  # Convert to 3-channel image

        out.write(img)

    # Release everything when job is finished
    out.release()

# Example usage
# Assuming you have a list of numpy arrays named 'all_frames'
# create_video(all_frames)

create_video(all_frames)

