{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-12T01:20:02.114093700Z",
     "start_time": "2023-11-12T01:20:01.393339700Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import time \n",
    "import PIL\n",
    "from IPython import display\n",
    "\n",
    "def get_training_data(image_path):\n",
    "    \"\"\"Loads an image from the given path into a numpy array.\"\"\"\n",
    "    with Image.open(image_path) as img:\n",
    "        return np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "df = pd.read_csv('./mTBI/eye_motion_trace/00029_U_4_19_2018_9_18_9_V001.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T01:20:02.132299Z",
     "start_time": "2023-11-12T01:20:01.409802400Z"
    }
   },
   "id": "f564f4bcfca454fa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Actual project"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e8ac1d16ef2b9ad"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def extract_frame(video_path, time_in_seconds):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file.\")\n",
    "        return None\n",
    "\n",
    "    # Get the frame rate of the video\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Calculate the frame number to be captured\n",
    "    frame_number = int(time_in_seconds * fps)\n",
    "\n",
    "    # Set the frame position\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "\n",
    "    # Read the frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "\n",
    "    if ret:\n",
    "        return frame  # Return the frame if successfully captured\n",
    "    else:\n",
    "        print(\"Error: Could not retrieve frame at given time.\")\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T01:20:02.171113200Z",
     "start_time": "2023-11-12T01:20:01.498888500Z"
    }
   },
   "id": "e940f9eb26493cfb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extracting data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28d3ec4acedf681c"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def convert_combine(arrays):\n",
    "    grayscaled = [np.mean(array, axis=2, keepdims=True) for array in arrays]\n",
    "    combined = np.stack(grayscaled, axis=0)\n",
    "    \n",
    "    # list of all the frames\n",
    "    \n",
    "    return combined\n",
    "    # extract_frame('./mTBI/video_sequence/00029_U_4_19_2018_9_18_9_V001.avi', 1).shape\n",
    "\n",
    "def avi_parser(avi_file=\"./mTBI/video_sequence/00029_U_4_19_2018_9_18_9_V001.avi\"):\n",
    "    temp = []\n",
    "    \n",
    "    # avi_file = \"00029_U_4_19_2018_9_18_9_V001.avi\"\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(avi_file)\n",
    "    \n",
    "    # Check if the video file opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file.\")\n",
    "    else:\n",
    "        # Loop through each frame\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break  # Break the loop if there are no frames left\n",
    "    \n",
    "            # Process the frame here\n",
    "            # For example, you can display the frame using cv2.imshow(\"Frame\", frame)\n",
    "            # print(type(frame))\n",
    "            temp.append(frame)\n",
    "    \n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return temp"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T01:20:02.237218300Z",
     "start_time": "2023-11-12T01:20:01.539199600Z"
    }
   },
   "id": "b50e3376866d84c6"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "(512, 512, 3)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avi_obj = avi_parser(\"./mTBI/video_sequence/00029_U_4_19_2018_9_18_9_V001.avi\")\n",
    "avi_obj[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T01:20:02.706429500Z",
     "start_time": "2023-11-12T01:20:01.558564100Z"
    }
   },
   "id": "8320974ce446ca3c"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d54f5f68916a336b"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "numpy.ndarray"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avi_train = convert_combine(avi_obj)\n",
    "type(avi_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T01:20:05.761790200Z",
     "start_time": "2023-11-12T01:20:02.109570400Z"
    }
   },
   "id": "2aa4767e4ef996e6"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def shape_finder(csv_directory= './mTBI/eye_motion_trace'):\n",
    "    # Directory containing CSV files\n",
    "    csv_directory = './mTBI/eye_motion_trace'\n",
    "    \n",
    "    # Loop through each file in the directory\n",
    "    for filename in os.listdir(csv_directory):\n",
    "        if filename.endswith('.csv'):\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(csv_directory, filename)\n",
    "    \n",
    "            # Read the CSV file into a DataFrame\n",
    "            temp = pd.read_csv(file_path)\n",
    "    \n",
    "            # Print the filename and shape of the DataFrame\n",
    "            print(f\"File: {filename}, Shape: {temp.shape[0]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T01:20:05.764141400Z",
     "start_time": "2023-11-12T01:20:05.750155200Z"
    }
   },
   "id": "56bcf7ecb580475e"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "(300, 512, 512, 1)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avi_train = avi_train.reshape(avi_train.shape[0], 512, 512, 1).astype('float32')\n",
    "avi_train_images = (avi_train - 127.5) / 127.5  # Normalize the images to [-1, 1]\n",
    "avi_train_images.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T01:20:06.267120200Z",
     "start_time": "2023-11-12T01:20:05.769724200Z"
    }
   },
   "id": "244416fbe94e5ed6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Getting noise from vectors"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af353ea73f45772f"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "df_array = df.drop(columns=['time[s]']).to_numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T01:20:06.290670400Z",
     "start_time": "2023-11-12T01:20:06.220383600Z"
    }
   },
   "id": "a8fc020823465b1f"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-9.97400e-01,  2.69573e+01, -1.22000e-02, ...,  2.70000e+01,\n        -1.22000e-02,  2.47200e-01],\n       [-9.97600e-01,  2.43766e+01, -1.22000e-02, ..., -1.00000e+00,\n        -1.22000e-02, -9.20000e-03],\n       [-9.97900e-01,  2.18332e+01, -1.22000e-02, ..., -1.00000e+00,\n        -1.22000e-02, -9.20000e-03],\n       ...,\n       [-1.84842e+01, -2.28521e+01, -2.25600e-01, ..., -8.00000e+00,\n        -2.31900e-01, -7.32000e-02],\n       [-1.85297e+01, -2.39695e+01, -2.26200e-01, ..., -8.00000e+00,\n        -2.31900e-01, -7.32000e-02],\n       [-1.85716e+01, -2.50233e+01, -2.26700e-01, ..., -3.50000e+01,\n        -2.31900e-01, -3.20400e-01]])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_array"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T01:20:06.363060400Z",
     "start_time": "2023-11-12T01:20:06.286654800Z"
    }
   },
   "id": "5036d26693502bd7"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-9.97400000e-01,  2.69573000e+01, -1.22000000e-02, ...,\n         5.63321910e-01,  7.99001740e-01,  3.94848256e-02],\n       [-9.97600000e-01,  2.43766000e+01, -1.22000000e-02, ...,\n         1.86606109e-01,  2.58139274e-01,  2.51103863e-02],\n       [-9.97900000e-01,  2.18332000e+01, -1.22000000e-02, ...,\n         3.24500055e-01,  4.81204305e-01,  8.70232442e-01],\n       ...,\n       [-1.84842000e+01, -2.28521000e+01, -2.25600000e-01, ...,\n         4.22212620e-02,  7.96284929e-01,  1.78540979e-01],\n       [-1.85297000e+01, -2.39695000e+01, -2.26200000e-01, ...,\n         8.84062961e-02,  9.85273430e-01,  5.26902718e-01],\n       [-1.85716000e+01, -2.50233000e+01, -2.26700000e-01, ...,\n         6.97287123e-01,  4.61467631e-01,  5.15684150e-01]])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of rows in the original array\n",
    "num_rows = df_array.shape[0]\n",
    "\n",
    "# Generate noise (random numbers, for example)\n",
    "# Adjust the parameters of np.random.rand() as needed\n",
    "noise = np.random.rand(num_rows, 92)\n",
    "\n",
    "# Concatenate the original array and the noise array\n",
    "combined_array = np.concatenate([df_array, noise], axis=1)\n",
    "combined_array"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T01:20:06.366508800Z",
     "start_time": "2023-11-12T01:20:06.318869500Z"
    }
   },
   "id": "d8c515049ad0c7a7"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7ac0b14d42355cae"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Side quest"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a7e2d50173f9fac"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 256"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T01:20:06.461395600Z",
     "start_time": "2023-11-12T01:20:06.347509400Z"
    }
   },
   "id": "620ab37d4386d976"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Batch and shuffle the data\n",
    "data_set = tf.data.Dataset.from_tensor_slices(avi_train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T01:20:06.536728100Z",
     "start_time": "2023-11-12T01:20:06.372206400Z"
    }
   },
   "id": "68a7aba298160288"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    # Start with a dense layer that reshapes into a 8x8x1024 tensor\n",
    "    model.add(layers.Dense(8*8*1024, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((8, 8, 1024)))\n",
    "    # Size becomes 8x8x1024 here\n",
    "\n",
    "    # Upsample to 16x16\n",
    "    model.add(layers.Conv2DTranspose(512, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    # Size becomes 16x16x512 here\n",
    "\n",
    "    # Upsample to 32x32\n",
    "    model.add(layers.Conv2DTranspose(256, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    # Size becomes 32x32x256 here\n",
    "\n",
    "    # Upsample to 64x64\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    # Size becomes 64x64x128 here\n",
    "\n",
    "    # Upsample to 128x128\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    # Size becomes 128x128x64 here\n",
    "\n",
    "    # Upsample to 256x256\n",
    "    model.add(layers.Conv2DTranspose(32, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    # Size becomes 256x256x32 here\n",
    "\n",
    "    # Upsample to 512x512\n",
    "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    # Final size becomes 512x512x1\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T01:20:06.561927900Z",
     "start_time": "2023-11-12T01:20:06.529020800Z"
    }
   },
   "id": "b69f40301d3a01fe"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "generator = make_generator_model()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T01:20:08.199269500Z",
     "start_time": "2023-11-12T01:20:06.550572200Z"
    }
   },
   "id": "6230a74fe1a435d3"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                     input_shape=[512, 512, 1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T01:20:08.214594Z",
     "start_time": "2023-11-12T01:20:08.200290100Z"
    }
   },
   "id": "3741eea94522d5b1"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "discriminator = make_discriminator_model()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T01:20:08.396084200Z",
     "start_time": "2023-11-12T01:20:08.214594Z"
    }
   },
   "id": "e2a3746c54151e"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T01:20:08.417633200Z",
     "start_time": "2023-11-12T01:20:08.402341Z"
    }
   },
   "id": "eef66a5ac39960ef"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# descrim loss function\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T01:20:08.447994100Z",
     "start_time": "2023-11-12T01:20:08.420148800Z"
    }
   },
   "id": "bfecdf220a4cd7fd"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# generator loss\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T01:20:08.473053900Z",
     "start_time": "2023-11-12T01:20:08.447994100Z"
    }
   },
   "id": "42230804b126c47c"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T01:20:08.526522800Z",
     "start_time": "2023-11-12T01:20:08.470734100Z"
    }
   },
   "id": "7fe77c12a4ae6430"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T01:20:08.551583200Z",
     "start_time": "2023-11-12T01:20:08.489389400Z"
    }
   },
   "id": "52f628d41db80919"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 1\n",
    "\n",
    "# You will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = combined_array[0]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T01:20:08.551583200Z",
     "start_time": "2023-11-12T01:20:08.502705600Z"
    }
   },
   "id": "d60dfd70b4da5256"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_images = generator(noise, training=True)\n",
    "      print(images.shape)\n",
    "      real_output = discriminator(images, training=True)\n",
    "      print('DEBUB NOISE:', noise)\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T01:20:08.555071800Z",
     "start_time": "2023-11-12T01:20:08.524504600Z"
    }
   },
   "id": "d2e4e3504790d15"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    for image_batch in dataset:\n",
    "      train_step(image_batch)\n",
    "    # Produce images for the GIF as you go\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator,\n",
    "                             epoch + 1,\n",
    "                             seed)\n",
    "    # Save the model every 15 epochs\n",
    "    if (epoch + 1) % 15 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "  # Generate after the final epoch\n",
    "  display.clear_output(wait=True)\n",
    "  generate_and_save_images(generator,\n",
    "                           epochs,\n",
    "                           seed)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T01:20:08.635381800Z",
     "start_time": "2023-11-12T01:20:08.555071800Z"
    }
   },
   "id": "f0852a5a2bda8928"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "  predictions = model(test_input, training=False)\n",
    "\n",
    "  fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(4, 4, i+1)\n",
    "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "      plt.axis('off')\n",
    "\n",
    "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-12T01:20:08.636957700Z",
     "start_time": "2023-11-12T01:20:08.575204500Z"
    }
   },
   "id": "e2a3e33ccdb29ad3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 512, 512, 1)\n",
      "DEBUB NOISE: Tensor(\"random_normal:0\", shape=(256, 100), dtype=float32)\n",
      "(256, 512, 512, 1)\n",
      "DEBUB NOISE: Tensor(\"random_normal:0\", shape=(256, 100), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train(data_set, EPOCHS)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-12T01:20:08.591891800Z"
    }
   },
   "id": "57291fbbd8922d0a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "dcc68a440041d891"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Display a single image using the epoch number\n",
    "def display_image(epoch_no):\n",
    "  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "96fc8e9fc5a17eee"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display_image(EPOCHS)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "42ed40286770e88e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
